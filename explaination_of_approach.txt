Tokenizer: I first scanned through the entire train.txt to generate a vocabulary. Functions, variables and digits are extracted. This allows us to add some domain knowledge to the model instead of just passing characters to the model. Also, a variable by any name is the same. So, variables are replaced with tokens such as var0, var1, var2,... var0 being the variable we differentiate on. This allows us to pass this variable easily to the model. Digits (0 - 9) are considered separately as tokens because using an entire number (like 1023) as token would not be wise as our test set is sure to contain numbers not present in training set.

I initially tried modeling this as a neural machine translation task with an lstm as an encoder and decoder.
This approach didn't give satisfactory results. From the results obtained, it is clear that the model did learn the structure of the output but struggled to generate the right coefficients. A more complex model with attention, bilstm and a few more fully connected layers could help with the coefficients

After this, I decided to try a simple Fully connected Neural Network with 4 layers (and 2 dropout layers). I padded the sequences with a padding token to make them all have a sequence length of 30. Encodings of all the vectors are concatenated and passed through the FCNN. The output we get is a concatenation of distributions for all the entries in the sequence. This was then trained with a cross entropy loss on each element of the sequence.

The model is trained for 100 epochs. All the final hyperparameters we settled on are available in train.py

We have a validation accuracy of 92.974% for our best model. The test accuracy for this model is 92.756%
